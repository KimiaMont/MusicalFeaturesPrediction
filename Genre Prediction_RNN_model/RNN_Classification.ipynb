{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre Classification Using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from model import SequenceClassification\n",
    "from utils import *\n",
    "from train_classifier import train_model\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "\n",
    "- The Data loader class helps in iterating through dataset in the training loops and allow infinite loop over the dataset\n",
    "- During each iteration, the dataset returns a tuple of torch.tensor object for temporal inputs, audio inputs, and target varibale\n",
    "- The target varbiale is processed with one hot encoder from scikit-learn preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetClassification(Dataset):\n",
    "\n",
    "    def __init__(self, ids):\n",
    "\n",
    "        self.temporal_features = pd.read_csv('data/Temporal_Features.csv')\n",
    "        self.temporal_features = normalize_temporal_features(\n",
    "            self.temporal_features)\n",
    "\n",
    "        self.audio_features = pd.read_csv(\n",
    "            'data/Audio_Features.csv').iloc[:, 1:]\n",
    "        self.audio_features = normalize_df(self.audio_features)\n",
    "\n",
    "        self.track_ids = ids\n",
    "        self.track_info = pd.read_csv('data/trackinfo.csv').iloc[:, 1:]\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.encoder.fit(self.track_info['genre_top'].values.reshape(-1, 1))\n",
    "        self.track_info = self.track_info[self.track_info['trackID'].isin(\n",
    "            self.track_ids)][['trackID', 'genre_top']]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.track_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        track_ids = self.track_ids[idx]\n",
    "        if isinstance(track_ids, np.int64):\n",
    "            track_ids = np.array([track_ids])\n",
    "        sample_temporal_features = self.temporal_features[self.temporal_features['trackID'].isin(\n",
    "            track_ids)].drop(labels='trackID', axis=1)\n",
    "        sample_audio_features = self.audio_features[self.audio_features['trackID'].isin(\n",
    "            track_ids)].drop(labels='trackID', axis=1)\n",
    "        sample_target = self.track_info[self.track_info['trackID'].isin(\n",
    "            track_ids)].drop(labels='trackID', axis=1)\n",
    "\n",
    "        sample_temporal_features = torch.tensor(\n",
    "            sample_temporal_features.values).reshape(224, 1)\n",
    "        sample_audio_features = torch.tensor(sample_audio_features.values)\n",
    "        sample_target = self.encoder.transform(sample_target.values).toarray()\n",
    "        sample_target = torch.tensor(sample_target)\n",
    "\n",
    "        return (sample_temporal_features, sample_audio_features), sample_target, track_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "- The model is implemented using PyTorch\n",
    "- The network has two branches.\n",
    "- The recurrent layers (Bi-directional LSTM cell) process the temporal sequence and outputs the last output of the LSTM cell.\n",
    "- Bi-directional LSTM cell process the sequence from left and right in order to fix the common issue in reccurent network which is forgetting earlier signals in the sequence\n",
    "- The Audio features go through a fully connected layer\n",
    "- The Bi-LSTM and fully connected output is concatenated and passed through a second fully connected layer \n",
    "- The dropout in this layers prevents over-fitting\n",
    "- The output has 16 nodes and the output logits are used for calculating cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassification(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, hidden_size=64, num_layers=2):\n",
    "\n",
    "        super(SequenceClassification, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.BiLSTM = nn.LSTM(input_size=1,\n",
    "                              hidden_size=self.hidden_size,\n",
    "                              num_layers=self.num_layers,\n",
    "                              dropout=0.5,\n",
    "                              bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(8, self.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.hidden_size + self.hidden_size * 2, self.hidden_size)\n",
    "        self.fc3 = nn.Linear(self.hidden_size, 16)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        temporal_features, audio_feature = inputs\n",
    "        temporal_features = temporal_features.permute(1, 0, 2).float()\n",
    "        audio_features = audio_feature.float()\n",
    "\n",
    "        h0 = Variable(torch.zeros(self.num_layers * 2,\n",
    "                                  self.batch_size, self.hidden_size)).float()\n",
    "        c0 = Variable(torch.zeros(self.num_layers * 2,\n",
    "                                  self.batch_size, self.hidden_size)).float()\n",
    "\n",
    "\n",
    "        lstm_output, (final_h0, final_c0) = self.BiLSTM(\n",
    "            temporal_features, None)\n",
    "\n",
    "        lstm_final_output = lstm_output[-1]\n",
    "\n",
    "        x = F.relu(self.fc1(audio_features))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        concat_featurs = torch.cat([lstm_final_output, x.reshape(-1, self.hidden_size)], dim=1)\n",
    "\n",
    "        x = F.relu(self.fc2(concat_featurs))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "- The training loop given number of epochs and saves the models weights for best accuracy over validation dataset.\n",
    "- The loss criterion is Cross Entorpy for multi-class classification \n",
    "- The optimizer is Stochiastic Gradient Descent optimizer with exponential scheduler which decreases the learning exponentially during training\n",
    "- Other auxiliary functions in help in inference on the test set and calculating test set accuaracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, dataloaders):\n",
    "\n",
    "    if os.path.isfile('classification_tained_model.pth'):\n",
    "        checkpoint = torch.load('classification_tained_model.pth')\n",
    "        model.load_state_dict(checkpoint)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    loss_df = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "\n",
    "            for inputs, target, ids in dataloaders[phase]:\n",
    "\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    temporal_data, audio_data = inputs\n",
    "                    temporal_data = temporal_data.to(device)\n",
    "                    audio_data = audio_data.to(device)\n",
    "\n",
    "                    outputs = model((temporal_data, audio_data))\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    outputs = outputs.squeeze()\n",
    "                    _, target = torch.max(target.squeeze(), 1)\n",
    "                    loss = criterion(outputs, target)\n",
    "\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                running_loss += loss.item() * inputs[0].size(0)\n",
    "                running_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            loss_df[phase + '_loss'].append(epoch_loss)\n",
    "            loss_df[phase + '_acc'].append(epoch_acc.item())\n",
    "\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "\n",
    "    loss_df = pd.DataFrame(loss_df)\n",
    "    ax = loss_df[['train_loss', 'val_loss']].plot(figsize=(15, 8))\n",
    "    ax.set_ylabel('Loss')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig('loss.pdf', dpi=300)\n",
    "\n",
    "    loss_df = pd.DataFrame(loss_df)\n",
    "    ax = loss_df[['train_acc', 'val_acc']].plot(figsize=(15, 8))\n",
    "    ax.set_ylabel('Acc')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig('Acc.pdf', dpi=300)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDF(object):\n",
    "    def __init__(self, pdf, size=(200,200)):\n",
    "    self.pdf = pdf\n",
    "    self.size = size\n",
    "\n",
    "    def _repr_html_(self):\n",
    "    return '<iframe src={0} width={1[0]} height={1[1]}></iframe>'.format(self.pdf, self.size)\n",
    "\n",
    "    def _repr_latex_(self):\n",
    "    return r'\\includegraphics[width=1.0\\textwidth]{{{0}}}'.format(self.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=loss.pdf width=1000 height=800></iframe>"
      ],
      "text/latex": [
       "\\includegraphics[width=1.0\\textwidth]{loss.pdf}"
      ],
      "text/plain": [
       "<__main__.PDF at 0x7f8e5383d860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF('loss.pdf', size=(1000, 800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=Acc.pdf width=1000 height=800></iframe>"
      ],
      "text/latex": [
       "\\includegraphics[width=1.0\\textwidth]{Acc.pdf}"
      ],
      "text/plain": [
       "<__main__.PDF at 0x139a5b65278>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF('Acc.pdf', size=(1000, 800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Training/validation loss difference shows that the model is overfitting \n",
    "- Adding regularization or increasing dropout rate can reduce the gap between training validtion loss (time consuming)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataframes and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "temp_feature_df = pd.read_csv('data/Temporal_Features.csv')\n",
    "audio_df = pd.read_csv('data/Audio_Features.csv')\n",
    "audio_df = audio_df.iloc[:, 1:]\n",
    "info_df = pd.read_csv('data/trackinfo.csv').iloc[:, 1:]\n",
    "\n",
    "\n",
    "track_ids = audio_df['trackID'].values\n",
    "shuffle(track_ids)\n",
    "len_ids = len(track_ids)\n",
    "\n",
    "train_ids = track_ids[np.arange(len_ids) < int(0.6 * len_ids)]\n",
    "val_ids = track_ids[(np.arange(len_ids) > int(0.6 * len_ids)) & (np.arange(len_ids) < int(0.9 * len_ids))]\n",
    "test_ids = track_ids[np.arange(len_ids) > int(0.9 * len_ids)]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataset = AudioDatasetClassification(train_ids)\n",
    "val_dataset = AudioDatasetClassification(val_ids)\n",
    "test_dataset = AudioDatasetClassification(test_ids)\n",
    "\n",
    "\n",
    "dataloaders = {'train': DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True),\n",
    "              'val': DataLoader(val_dataset, batch_size=BATCH_SIZE, drop_last=True),\n",
    "              'test': DataLoader(test_dataset, batch_size=1, drop_last=True, shuffle=False)}\n",
    "\n",
    "dataset_sizes = {'train': len(train_dataset),\n",
    "                 'val': len(val_dataset),\n",
    "                 'test': len(test_dataset)}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating model, loss criterion, optimizer and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceClassification(batch_size=BATCH_SIZE, hidden_size=256).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(lr=0.001, momentum=0.9, params=model.parameters())\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for few epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, criterion, optimizer, exp_lr_scheduler, 20, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), 'classification_tained_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on test-set and saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_classification(trained_model, dataloaders['test'])\n",
    "preds.to_csv('classification_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
